{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaraAljuraybah/Data-Mining-Project/blob/main/Reports/Phase2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 2: Data Summarization and Pre-processing"
      ],
      "metadata": {
        "id": "Ie7749R8g7Km"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "T2rxd8f-Lzy4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(style=\"whitegrid\")\n",
        "df = pd.read_excel(\"Raw_dataset.xlsx\")\n",
        "print(\"Shape:\", df.shape)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "B8SC-G_8M5kW",
        "outputId": "78896d3c-40a0-4519-cc17-23faea7e3596"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Raw_dataset.xlsx'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2148942418.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"whitegrid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Raw_dataset.xlsx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         io = ExcelFile(\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1551\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m     ) as handle:\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Raw_dataset.xlsx'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 . Data Analysis\n",
        "****\n",
        "**1.1 Statical Summary**"
      ],
      "metadata": {
        "id": "BmKqv9DK-Lop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()\n"
      ],
      "metadata": {
        "id": "MTitQeuxPhiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The statistical summary provides an overview of the dataset. For example, the Age attribute shows an average of around 18 years, which is reasonable since the dataset represents undergraduate students. However, the maximum value of 48 years indicates the presence of an outlier that should be considered during preprocessing.\n",
        "\n",
        "Some attributes such as Gender and Name of College are shown as numerical values in the summary, but in reality, they are categorical features that have been encoded using numbers. Therefore, their mean and standard deviation are not meaningful and should be interpreted through frequency counts instead."
      ],
      "metadata": {
        "id": "b_7VyQBZ4GHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.2 Missing values**"
      ],
      "metadata": {
        "id": "8QG1hfD2-mOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "mp8KRzV1DPoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The missing values analysis shows that there are no null or missing entries in the dataset. This is an advantage since it ensures the dataset is complete and reduces the need for imputation or dropping rows/columns. Having a complete dataset allows us to focus on other preprocessing tasks such as handling outliers, encoding categorical features, and normalization."
      ],
      "metadata": {
        "id": "3ksawTvsEaO4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**1.3 Plots**\n",
        "- Histogram (Age)"
      ],
      "metadata": {
        "id": "zzU2WyWu-3hZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(df[\"Age\"], bins=20, kde=True, color=\"skyblue\")\n",
        "plt.title(\"Distribution of Age\", fontsize=14)\n",
        "plt.xlabel(\"Age\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "muO85N7DEhra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The histogram above shows the distribution of students’ ages. Most students are between 17 and 20 years old, which is expected for undergraduate levels. The data is slightly right-skewed, meaning there are a few older participants aged above 25. This pattern confirms that the dataset mainly represents young students, while the presence of older ages (up to 48) might indicate outliers that should be checked during the preprocessing phase.\n",
        "\n"
      ],
      "metadata": {
        "id": "n2kquPSoiqgL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Boxplot (Age)"
      ],
      "metadata": {
        "id": "xFZ97CPH_EYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "sns.boxplot(x=df[\"Age\"], color=\"lightcoral\")\n",
        "plt.title(\"Boxplot of Age\", fontsize=14)\n",
        "plt.xlabel(\"Age\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RWrLUiZcKtBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The boxplot of the Age attribute shows that the majority of students are between 17 and 20 years old, with a median age of 18. Several outliers are present, including values above 25 and one extreme case at 48 years, which may indicate data entry errors or older students. These outliers should be carefully considered during preprocessing."
      ],
      "metadata": {
        "id": "HfWx45LdLZWz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Barplot (gender)\n"
      ],
      "metadata": {
        "id": "9ty41tIT_LEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(data=df, x=\"Gender\", hue=\"Gender\", palette=\"Set2\", legend=False)\n",
        "plt.title(\"Distribution of Gender\", fontsize=14)\n",
        "plt.xlabel(\"Gender (1 = Male, 2 = Female)\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "A8oT3UsQLacY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bar plot of the Gender attribute shows the distribution of male and female students. The dataset indicates that there are more male students compared to female students. However, the appearance of an unexpected value (4) suggests the presence of data entry or coding errors that need to be addressed during preprocessing."
      ],
      "metadata": {
        "id": "5jxUgHVNQxk8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Barplot (Use Instagram)"
      ],
      "metadata": {
        "id": "-EHto8AImUr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(data=df, x=\"Use Instagram\", hue=\"Use Instagram\", palette=\"pastel\", legend=False)\n",
        "plt.title(\"Distribution of Instagram Usage\", fontsize=14)\n",
        "plt.xlabel(\"Use Instagram (0 = No, 1 = Yes)\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DwwvI9GFQyyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plot shows Instagram usage distribution: most students use Instagram (1), while fewer do not (0). One invalid entry (11) was detected, which should be corrected during preprocessing."
      ],
      "metadata": {
        "id": "U6-dX4G4Slwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Unique values in Use Instagram:\", df[\"Use Instagram\"].unique())\n",
        "\n",
        "invalid_count = df[df[\"Use Instagram\"] == 11].shape[0]\n",
        "print(\"Number of invalid entries (11):\", invalid_count)"
      ],
      "metadata": {
        "id": "uYWnx-gTS03A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.4 Class label distribution**"
      ],
      "metadata": {
        "id": "UicZyOn-jBWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(7,4))\n",
        "sns.countplot(data=df, x=\"Impact on Academic Performance\", hue=\"Impact on Academic Performance\", palette=\"viridis\", legend=False)\n",
        "plt.title(\"Distribution of Impact on Academic Performance\", fontsize=14)\n",
        "plt.xlabel(\"Impact on Academic Performance (1 = Very Low → 5 = Very High, 0 = Invalid)\", fontsize=10)\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n",
        "print(df[\"Impact on Academic Performance\"].value_counts())\n"
      ],
      "metadata": {
        "id": "mcYGubBjWPtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plot shows the distribution of the class label Impact on Academic Performance. Most students fall between levels 2 to 5, with the highest counts at level 3 (747) and level 5 (713). Fewer students are at levels 1 (561) and 2 (462), while only 22 invalid entries (0) were detected. These invalid values will need to be handled during preprocessing."
      ],
      "metadata": {
        "id": "NpXQoMsCW4fw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Data Pre-processing\n",
        "****"
      ],
      "metadata": {
        "id": "RiCQWqxInm9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "df_raw = pd.read_excel(\"Raw_dataset.xlsx\")\n",
        "df=df_raw.copy()\n",
        "print(\"shape:\",df.shape)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "qq-vYzQkW7W9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1 Duplicates Removal (Cleaning data)**"
      ],
      "metadata": {
        "id": "8ZC9u2KhqpCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #first we cheak for duplicates\n",
        "dup_count = df.duplicated().sum()\n",
        "print(\"Number of duplicates: \",dup_count)\n",
        "\n",
        "print(\"Shape before removing duplicates: \", df.shape)\n",
        "#removing the duplicates\n",
        "df = df.drop_duplicates()\n",
        "print(\"Shape after removing duplicates: \", df.shape)"
      ],
      "metadata": {
        "id": "uNUCfA2V8U8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We detected 166 duplicate rows and removed them using drop_duplicates(). This step prevents redundancy and ensures each record is unique. Result: Dataset reduced from 3028 to 2862 rows\n",
        "\n",
        "- First we checked unique values\n",
        "- Detecting invalid values is important to correct data entry errors and improve reliability\n",
        "\n",
        "- Result: Marked invalid values for cleaning in the next step\n",
        "\n"
      ],
      "metadata": {
        "id": "CRzvjb3d3oCj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Variable transformation**\n",
        "\n",
        "Variable transformation is the process of modifying data values to improve consistency and prepare them for analysis. It helps ensure that all features are on comparable scales and that patterns become clearer.\n",
        "\n",
        " In this project, transformations such as discretization and normalization were applied to selected columns to standardize ranges (e.g., converting 0–5 scales into 0–1)to help making the data more suitable for interpretation and modeling.\n"
      ],
      "metadata": {
        "id": "vmLc7l9qrkpx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Discretization**"
      ],
      "metadata": {
        "id": "16N3-yxAr-U9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "discret= 'Age'\n",
        "numOfBins=10\n",
        "df['discretized'] =pd.cut(df[discret], bins=numOfBins, labels=False)\n",
        "print(df[['Age', 'discretized']])\n"
      ],
      "metadata": {
        "id": "ZSCuzXXj8RB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "in this step, the 'Age' column was discretized using the pd.cut() function to divide continuous numeric values into fixed intervals (bins).\n",
        "\n",
        "This transformation groups similar age values together, simplifying the data and making patterns easier to identify.\n",
        "\n",
        "\n",
        "The resulting “discretized” column assigns each age to a specific category (e.g., 0, 1, 2), which helps in comparing and analyzing the data more effectively during later stages of modeling."
      ],
      "metadata": {
        "id": "4fKrlpdfsUPk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Square root transformation**"
      ],
      "metadata": {
        "id": "qWWjVQwkszUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df['Time_spent_sqrt']=np.sqrt(df['Time Spent'])\n",
        "print(df[['Time Spent', 'Time_spent_sqrt']])"
      ],
      "metadata": {
        "id": "tVSbb8Oj_Z8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A square root transformation was applied to the 'Time Spent' column, which represents a 0–5 scale.\n",
        "This transformation reduces the effect of higher scale values and smooths out differences between responses, making the distribution more balanced while keeping the overall meaning of the data the same."
      ],
      "metadata": {
        "id": "lpeIMFoDs8NH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Z-score normalization**"
      ],
      "metadata": {
        "id": "XKyhLPXntSbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler= StandardScaler()\n",
        "\n",
        "df['Frequency of Use_zscore']=scaler.fit_transform(df[['Frequency of Use']])\n",
        "print(df[['Frequency of Use', 'Frequency of Use_zscore']])"
      ],
      "metadata": {
        "id": "mcpo_FtUnZ_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Z-score normalization was applied to the 'Frequency of Use'\n",
        "column to standardize its values. This transformation centers the data around a mean of 0 and a standard deviation of 1, This makes the data easier to compare with other variables and prevents scale differences from affecting the analysis.\n",
        "\n",
        "so every value in the column becomes:\n",
        "\n",
        "\t•\t0 → the average use frequency.\n",
        "\n",
        "\t•\tpositive → above average.\n",
        "\n",
        "\t•\tnegative → below average."
      ],
      "metadata": {
        "id": "tU-D5a-5ttEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Min-Max normalization**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "I604FR58u7ld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "sacler= MinMaxScaler()\n",
        "df[['Impact on Academic Performance_scaled']]=sacler.fit_transform(df[['Impact on Academic Performance']])\n",
        "print(df[['Impact on Academic Performance', 'Impact on Academic Performance_scaled']])"
      ],
      "metadata": {
        "id": "dwLuSxU6n-QY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 'Impact on Academic Performance' column was normalized using the Min–Max scaling technique, which transforms all values into a range between 0 and 1.\n",
        "\n",
        "This method ensures consistency across numerical features, allowing fair comparison and improving the performance of upcoming analysis and modeling steps."
      ],
      "metadata": {
        "id": "upYaVuL2vEo5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2 Invalid Values Detection (Cleaning data)**"
      ],
      "metadata": {
        "id": "zpkxcNcTrESW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#invalid values\n",
        "print(\"Unique values in Gender: \",df[\"Gender\"].unique())\n",
        "print(\"Unique values in Use Instagram: \",df[\"Use Instagram\"].unique())\n",
        "print(\"Unique values in Impact on Academic Performance: \",df[\"Impact on Academic Performance\"].unique())"
      ],
      "metadata": {
        "id": "mbB_0g6_8V92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Invalid entries in Gender, Use Instagram, and Impact on Academic Performance were replaced with NaN. This prepares the dataset for consistent imputation instead of treating wrong values as valid.\n",
        "\n",
        "- Result: Columns now contain only valid categories plus NaN"
      ],
      "metadata": {
        "id": "RzXfSLNQ8vhM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handle Invalid Values**"
      ],
      "metadata": {
        "id": "poNE57NMrPAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# invalid values handling\n",
        "\n",
        "# Gender\n",
        "df.loc[~df[\"Gender\"].isin([1,2]) , \"Gender\" ] = np.nan\n",
        "\n",
        "# Use Instagram\n",
        "df.loc[~df[\"Use Instagram\"].isin([0,1]) , \"Use Instagram\" ] = np.nan\n",
        "\n",
        "# Impact on Academic Performance\n",
        "df.loc[~df[\"Impact on Academic Performance\"].isin([1,2,3,4,5]) , \"Impact on Academic Performance\" ] = np.nan\n",
        "\n",
        "\n",
        "#printing the values for duble checking\n",
        "print(\"Unique values in Gender: \",df[\"Gender\"].unique())\n",
        "print(\"Unique values in Use Instagram: \",df[\"Use Instagram\"].unique())\n",
        "print(\"Unique values in Impact on Academic Performance: \",df[\"Impact on Academic Performance\"].unique())"
      ],
      "metadata": {
        "id": "3RuKpB8C8uiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "احس يبيلها شرح"
      ],
      "metadata": {
        "id": "BgLn8F6OhagB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "☹"
      ],
      "metadata": {
        "id": "erYBxetRUfB_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imputation -Binary- (mode/ median)**"
      ],
      "metadata": {
        "id": "x6IVY2nlrewr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# impute Gender and Use Instagram with mode\n",
        "for col in [\"Gender\",\"Use Instagram\"]:\n",
        "    mode_val = df[col].mode()[0]\n",
        "    df[col] = df[col].fillna(mode_val)\n",
        "    print(col, \"NaN filled with mode:\", mode_val)\n",
        "\n",
        "# impute Impact on Academic Performance with median (rounded)\n",
        "median_val = round(df[\"Impact on Academic Performance\"].median())\n",
        "df[\"Impact on Academic Performance\"] = df[\"Impact on Academic Performance\"].fillna(median_val)\n",
        "print(\"Impact on Academic Performance NaN filled with median:\", median_val)"
      ],
      "metadata": {
        "id": "T1SHCoTw84nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We used mode to impute missing values in categorical columns (such as Gender and Use Instagram), because it represents the most frequent value and preserves the original distribution of the data\n",
        "\n",
        "- For the target column (Impact on Academic Performance), we used the median since it is more suitable for ordinal data (Likert scale) and is less affected by outliers. We also applied the round() function to ensure the imputed value is an integer between 1 and 5"
      ],
      "metadata": {
        "id": "envrqOBp85v_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Outliers Handling -Numric-(Age - IQR)**"
      ],
      "metadata": {
        "id": "AZnOBoOmrqIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Q1= df[\"Age\"].quantile(0.25)\n",
        "Q3= df[\"Age\"].quantile(0.75)\n",
        "IQR = Q3 -Q1\n",
        "lower_bound = Q1-1.5 *IQR\n",
        "upper_bound = Q3 + 1.5 *IQR\n",
        "\n",
        "\n",
        "df_cleaned =df[(df[\"Age\"]>= lower_bound)&(df[\"Age\"]<= upper_bound)]\n",
        "print(\"Shape Before removing :\",df.shape)\n",
        "print(\"Shape After removing :\", df_cleaned.shape)"
      ],
      "metadata": {
        "id": "6tkZKWpz9gI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We applied the IQR method to detect and remove outliers in the Age column. A total of 84 records were identified as outliers (ages above 23, including an extreme value of 48). These rows were removed to reduce noise and ensure that the dataset better represents the actual distribution of students. The dataset size decreased slightly, but this has minimal impact compared to the benefit of having cleaner data"
      ],
      "metadata": {
        "id": "fMWtpKxj9hAD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----جزئية ساره من هنا -----"
      ],
      "metadata": {
        "id": "zONMChF4VENR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.4 Manual feature selection**"
      ],
      "metadata": {
        "id": "2zs5d6ub2Kpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "desired_cols = [\n",
        "    \"Age\",\n",
        "    \"Gender\",\n",
        "    \"Year of Study\",\n",
        "    \"Use Instagram\",\n",
        "    \"Use Twitter\",\n",
        "    \"Use Snapchat\",\n",
        "    \"Use Tiktok\",\n",
        "    \"Time Spent\",\n",
        "    \"Academic Purpose\",\n",
        "    \"Entertainment\",\n",
        "    \"Social Interaction\",\n",
        "    \"Addiction\",\n",
        "    \"Difficulty in Concentrating on Studies\",\n",
        "    \"Impact on Academic Performance\",  # target\n",
        "]\n",
        "\n",
        "\n",
        "existing = [c for c in desired_cols if c in df.columns]\n",
        "missing  = [c for c in desired_cols if c not in df.columns]\n",
        "\n",
        "print(\"Shape before manual selection:\", df.shape)\n",
        "print(\"Missing columns (check spelling / caps):\", missing)\n",
        "\n",
        "df_model = df[existing].copy()\n",
        "\n",
        "print(\"Shape after manual selection :\", df_model.shape)\n",
        "print(\"Final kept columns:\", list(df_model.columns))\n"
      ],
      "metadata": {
        "id": "zKPEb6rw2XjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_model.head()"
      ],
      "metadata": {
        "id": "b-QV0v2Q2z04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.5 Encode categorical features**"
      ],
      "metadata": {
        "id": "UT1ZsnESr-fA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KmmZIZuk2xbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We converted categorical variables into numeric dummy columns to be usable by ML models.\n"
      ],
      "metadata": {
        "id": "Uii-sIu5w-6b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.8 Standardization (for clustering & models sensitive to\n",
        "scale)**"
      ],
      "metadata": {
        "id": "zIey8IA7xEKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.9 Save final datasets**"
      ],
      "metadata": {
        "id": "zF3rdWHKxQDt"
      }
    }
  ]
}